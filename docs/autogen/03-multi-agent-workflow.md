# å¤šæ™ºèƒ½ä½“åä½œå®æˆ˜

## é¡¹ç›®å·¥ä½œæµç¨‹

æœ¬é¡¹ç›®å®ç°äº† 4 ä¸ªæ™ºèƒ½ä½“çš„åä½œæµç¨‹ï¼š

```
éœ€æ±‚åˆ†æå¸ˆ â†’ æµ‹è¯•å·¥ç¨‹å¸ˆ â†’ è´¨é‡è¯„å®¡ä¸“å®¶ â†’ æµ‹è¯•è¡¥å…¨å·¥ç¨‹å¸ˆ
```

## generate_reply åŸºç¡€ç”¨æ³•

```python
from autogen import AssistantAgent

agent = AssistantAgent(
    name="analyst",
    system_message="ä½ æ˜¯éœ€æ±‚åˆ†æå¸ˆ",
    llm_config={"config_list": [{"model": "qwen-plus", "api_key": "sk-xxx"}]}
)

# å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤
reply = agent.generate_reply(
    messages=[{"role": "user", "content": "åˆ†æè¿™ä¸ªéœ€æ±‚..."}]
)
print(reply)
```

## é¡¹ç›®å®æˆ˜ï¼š4 é˜¶æ®µå·¥ä½œæµ

ä» `backend/app/llm/autogen_runner.py` æå–ï¼š

###  1. éœ€æ±‚åˆ†æé˜¶æ®µ

```python
def run_requirement_analysis(document_data: list[dict]):
    system_message = (
        "ä½ æ˜¯ä¸€ä½èµ„æ·±éœ€æ±‚åˆ†æå¸ˆã€‚è¯·ä»”ç»†é˜…è¯»éœ€æ±‚æ–‡æ¡£ï¼Œ"
        "è¯†åˆ«å¹¶æå–æ–‡æ¡£ä¸­çš„æ‰€æœ‰å…·ä½“åŠŸèƒ½æ¨¡å—ã€ä¸šåŠ¡åœºæ™¯å’Œä¸šåŠ¡è§„åˆ™ã€‚"
    )
    
    # æ„å»ºæç¤ºè¯
    documents_text = "\n".join([doc["content"] for doc in document_data])
    prompt = f"è¯·åˆ†æä»¥ä¸‹éœ€æ±‚æ–‡æ¡£:\n{documents_text}"
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    analyst = AssistantAgent(
        name="requirement_analyst",
        system_message=system_message,
        llm_config={"config_list": [{"model": "qwen3-vl-flash"}]},
        human_input_mode="NEVER",
    )
    
    # ç”Ÿæˆå›å¤
    reply = analyst.generate_reply(
        messages=[{"role": "user", "content": prompt}]
    )
    
    # æå– JSON ç»“æœ
    analysis_result = extract_json(reply)
    return analysis_result
```

### 2. æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆé˜¶æ®µ

```python
def run_test_generation(analysis_result: dict):
    system_message = (
        "ä½ æ˜¯ä¸€ä½èµ„æ·±æµ‹è¯•å·¥ç¨‹å¸ˆã€‚æ ¹æ®éœ€æ±‚åˆ†æç»“æœï¼Œ"
        "ä¸ºæ¯ä¸ªåŠŸèƒ½æ¨¡å—ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹ã€‚ä»¥Markdownæ ¼å¼è¾“å‡ºã€‚"
    )
    
    prompt = f"æ ¹æ®ä»¥ä¸‹éœ€æ±‚ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹:\n{json.dumps(analysis_result)}"
    
    test_engineer = AssistantAgent(
        name="test_engineer",
        system_message=system_message,
        llm_config={"config_list": [{"model": "qwen3-next-80b"}]},
    )
    
    test_cases = test_engineer.generate_reply(
        messages=[{"role": "user", "content": prompt}]
    )
    
    return test_cases
```

### 3. è´¨é‡è¯„å®¡é˜¶æ®µ

```python
def run_quality_review(test_cases: str):
    system_message = (
        "ä½ æ˜¯è´¨é‡è¯„å®¡ä¸“å®¶ã€‚ä»”ç»†è¯„å®¡æµ‹è¯•ç”¨ä¾‹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼Œ"
        "ä»¥Markdownæ ¼å¼è¾“å‡ºè¯„å®¡æŠ¥å‘Šã€‚"
    )
    
    reviewer = AssistantAgent(
        name="quality_reviewer",
        system_message=system_message,
        llm_config={"config_list": [{"model": "qwen3-next-80b"}]},
    )
    
    review_report = reviewer.generate_reply(
        messages=[{"role": "user", "content": f"è¯„å®¡ä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹:\n{test_cases}"}]
    )
    
    return review_report
```

### 4. ç”¨ä¾‹è¡¥å…¨é˜¶æ®µ

```python
def run_test_completion(test_cases: str, review_report: str):
    system_message = (
        "ä½ æ˜¯ä¸€ä½æµ‹è¯•è¡¥å…¨å·¥ç¨‹å¸ˆã€‚æ ¹æ®è´¨é‡è¯„å®¡å‘ç°çš„ç¼ºå£ä¸å»ºè®®ï¼Œ"
        "ä»¥Markdownæ ¼å¼è¡¥å……ç¼ºå¤±çš„æµ‹è¯•ç”¨ä¾‹ã€‚"
    )
    
    completer = AssistantAgent(
        name="test_completer",
        system_message=system_message,
        llm_config={"config_list": [{"model": "qwen3-next-80b"}]},
    )
    
    additional_cases = completer.generate_reply(
        messages=[{"role": "user", "content": f"åŸæµ‹è¯•ç”¨ä¾‹:\n{test_cases}\n\nè¯„å®¡æŠ¥å‘Š:\n{review_report}"}]
    )
    
    return additional_cases
```

## å®Œæ•´å·¥ä½œæµç¼–æ’

ä» `backend/app/orchestrator/workflow.py` æå–ï¼š

```python
class SessionWorkflowExecution:
    async def execute(self):
        # 1. éœ€æ±‚åˆ†æ
        analysis_result = await run_requirement_analysis(document_data)
        await self._emit_progress("éœ€æ±‚åˆ†æå®Œæˆ", progress=0.3)
        
        # 2. æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
        test_cases = await run_test_generation(analysis_result)
        await self._emit_progress("æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå®Œæˆ", progress=0.6)
        
        # 3. è´¨é‡è¯„å®¡
        review_report = await run_quality_review(test_cases)
        await self._emit_progress("è´¨é‡è¯„å®¡å®Œæˆ", progress=0.8)
        
        # 4. ç”¨ä¾‹è¡¥å…¨
        additional_cases = await run_test_completion(test_cases, review_report)
        await self._emit_progress("ç”¨ä¾‹è¡¥å…¨å®Œæˆ", progress=0.9)
        
        # 5. åˆå¹¶ç»“æœ
        final_result = merge_test_cases(test_cases, additional_cases)
        return final_result
```

## JSON æå–æŠ€å·§

é¡¹ç›®ä¸­çš„ `_extract_json` å‡½æ•°ï¼š

```python
import json

def extract_json(content: str) -> dict:
    """ä» LLM å›å¤ä¸­æå– JSON"""
    try:
        start = content.index("{")
        end = content.rfind("}") + 1
        json_str = content[start:end]
        return json.loads(json_str)
    except Exception as e:
        logger.error(f"JSON æå–å¤±è´¥: {e}")
        return {"error": "è§£æå¤±è´¥", "raw": content[:500]}
```

## æµ‹è¯•ç”¨ä¾‹åˆå¹¶

é¡¹ç›®ä¸­çš„ `_merge_test_cases` å‡½æ•°ï¼š

```python
def merge_test_cases(base: dict, supplement: dict) -> dict:
    """åˆå¹¶åŸºç¡€æµ‹è¯•ç”¨ä¾‹å’Œè¡¥å……ç”¨ä¾‹ï¼Œå»é‡"""
    merged = {"modules": []}
    
    for source in (base, supplement):
        for module in source.get("modules", []):
            existing_module = next(
                (m for m in merged["modules"] if m["name"] == module["name"]),
                None
            )
            
            if existing_module:
                # åˆå¹¶åˆ°å·²æœ‰æ¨¡å—ï¼Œå»é‡
                existing_ids = {case["id"] for case in existing_module["cases"]}
                for case in module["cases"]:
                    if case["id"] not in existing_ids:
                        existing_module["cases"].append(case)
            else:
                # æ–°æ¨¡å—
                merged["modules"].append(module)
    
    return merged
```

## æµå¼è¾“å‡ºï¼ˆè¿›é˜¶ï¼‰

ä» `backend/app/llm/autogen_runner.py` æå–ï¼š

```python
from openai import OpenAI

def generate_streaming(system_message: str, prompt: str, on_chunk=None):
    """æµå¼ç”Ÿæˆï¼Œé€å—å›è°ƒ"""
    client = OpenAI(api_key="sk-xxx", base_url="https://...")
    
    stream = client.chat.completions.create(
        model="qwen-plus",
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": prompt},
        ],
        stream=True,
    )
    
    full_content = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            text = chunk.choices[0].delta.content
            full_content += text
            if on_chunk:
                on_chunk(text)  # å®æ—¶å›è°ƒ
    
    return full_content

# ä½¿ç”¨ç¤ºä¾‹
def my_callback(text):
    print(text, end="", flush=True)

result = generate_streaming(
    system_message="ä½ æ˜¯æµ‹è¯•å·¥ç¨‹å¸ˆ",
    prompt="ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹",
    on_chunk=my_callback
)
```

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
try:
    reply = agent.generate_reply(messages=[...])
    result = extract_json(reply)
except Exception as e:
    logger.error(f"æ™ºèƒ½ä½“è°ƒç”¨å¤±è´¥: {e}")
    result = {"error": str(e)}
```

### 2. è¶…æ—¶æ§åˆ¶

```python
llm_config = {
    "config_list": [...],
    "timeout": 300,  # å¤æ‚ä»»åŠ¡ç»™è¶³æ—¶é—´
}
```

### 3. æ—¥å¿—è®°å½•

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("å¼€å§‹éœ€æ±‚åˆ†æ...")
reply = agent.generate_reply(...)
logger.info(f"åˆ†æå®Œæˆï¼Œç»“æœé•¿åº¦: {len(reply)}")
```

## ä¸‹ä¸€æ­¥

ğŸ“™ [è¿›é˜¶æŠ€å·§ä¸æœ€ä½³å®è·µ](./04-advanced-tips.md)
